{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Entender la diferencia entre transformaciones y acciones en Spark\n",
    "- Aprender las operaciones b√°sicas con DataFrames\n",
    "- Practicar lectura y escritura de datos\n",
    "- Comprender el concepto de \"lazy evaluation\"\n",
    "\n",
    "## Prerequisitos\n",
    "- Haber completado `01_environment_check.ipynb`\n",
    "- Entorno de Spark funcionando\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 30 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 9: Big Data Processing (EMR, Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. Inicializar Spark\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Antes de trabajar con Spark, siempre necesitamos crear una sesi√≥n. Es como abrir una aplicaci√≥n antes de usarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos SparkSession para crear nuestra conexion con Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importamos funciones de Spark que usaremos frecuentemente\n",
    "# El alias 'F' es una convencion comun para acceder a las funciones\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Importamos tipos de datos para definir esquemas\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Creamos la sesion de Spark\n",
    "# master(\"local[*]\") usa todos los cores disponibles en modo local\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkBasics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reducimos la verbosidad de los logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark inicializado correctamente\")\n",
    "print(f\"Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Crear DataFrames\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Hay varias formas de crear un DataFrame en Spark:\n",
    "1. **Desde listas de Python** - Para datos peque√±os o pruebas\n",
    "2. **Desde archivos** - CSV, JSON, Parquet, etc.\n",
    "3. **Desde bases de datos** - JDBC, Hive, etc.\n",
    "\n",
    "**Analog√≠a del mundo real:** Crear un DataFrame es como preparar una hoja de c√°lculo. Puedes escribir los datos a mano, importarlos de un archivo, o conectarte a un sistema externo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METODO 1: Crear DataFrame desde una lista de tuplas\n",
    "# Cada tupla es una fila de datos\n",
    "\n",
    "# Datos de ventas de ejemplo\n",
    "ventas_data = [\n",
    "    (\"2024-01-01\", \"Laptop\", \"Electronica\", 2, 15000.0),\n",
    "    (\"2024-01-01\", \"Mouse\", \"Electronica\", 10, 350.0),\n",
    "    (\"2024-01-02\", \"Escritorio\", \"Muebles\", 1, 5000.0),\n",
    "    (\"2024-01-02\", \"Silla\", \"Muebles\", 4, 2500.0),\n",
    "    (\"2024-01-03\", \"Monitor\", \"Electronica\", 3, 8000.0),\n",
    "    (\"2024-01-03\", \"Teclado\", \"Electronica\", 8, 800.0),\n",
    "    (\"2024-01-04\", \"Lampara\", \"Muebles\", 5, 600.0),\n",
    "    (\"2024-01-04\", \"Laptop\", \"Electronica\", 1, 15000.0),\n",
    "    (\"2024-01-05\", \"Mouse\", \"Electronica\", 15, 350.0),\n",
    "    (\"2024-01-05\", \"Silla\", \"Muebles\", 2, 2500.0)\n",
    "]\n",
    "\n",
    "# Nombres de las columnas\n",
    "ventas_columnas = [\"fecha\", \"producto\", \"categoria\", \"cantidad\", \"precio_unitario\"]\n",
    "\n",
    "# Creamos el DataFrame\n",
    "df_ventas = spark.createDataFrame(ventas_data, ventas_columnas)\n",
    "\n",
    "# Mostramos las primeras filas\n",
    "print(\"DataFrame de ventas:\")\n",
    "df_ventas.show()\n",
    "\n",
    "# Output esperado:\n",
    "# +----------+----------+-----------+--------+---------------+\n",
    "# |     fecha|  producto|  categoria|cantidad|precio_unitario|\n",
    "# +----------+----------+-----------+--------+---------------+\n",
    "# |2024-01-01|    Laptop|Electronica|       2|        15000.0|\n",
    "# |2024-01-01|     Mouse|Electronica|      10|          350.0|\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METODO 2: Crear DataFrame con esquema explicito\n",
    "# Esto da mas control sobre los tipos de datos\n",
    "\n",
    "# Definimos el esquema usando StructType y StructField\n",
    "# StructField(nombre, tipo, puede_ser_nulo)\n",
    "esquema_clientes = StructType([\n",
    "    StructField(\"cliente_id\", StringType(), False),    # No puede ser nulo\n",
    "    StructField(\"nombre\", StringType(), True),         # Puede ser nulo\n",
    "    StructField(\"edad\", IntegerType(), True),\n",
    "    StructField(\"ciudad\", StringType(), True),\n",
    "    StructField(\"credito_limite\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Datos de clientes\n",
    "clientes_data = [\n",
    "    (\"C001\", \"Ana Garcia\", 28, \"CDMX\", 50000.0),\n",
    "    (\"C002\", \"Carlos Lopez\", 35, \"Guadalajara\", 75000.0),\n",
    "    (\"C003\", \"Maria Rodriguez\", 42, \"Monterrey\", 100000.0),\n",
    "    (\"C004\", \"Juan Martinez\", 31, \"CDMX\", 60000.0),\n",
    "    (\"C005\", \"Laura Sanchez\", 25, \"Puebla\", 40000.0)\n",
    "]\n",
    "\n",
    "# Creamos el DataFrame con el esquema definido\n",
    "df_clientes = spark.createDataFrame(clientes_data, esquema_clientes)\n",
    "\n",
    "# Mostramos el esquema para verificar los tipos\n",
    "print(\"Esquema del DataFrame de clientes:\")\n",
    "df_clientes.printSchema()\n",
    "\n",
    "# Mostramos los datos\n",
    "df_clientes.show()\n",
    "\n",
    "# Output esperado:\n",
    "# root\n",
    "#  |-- cliente_id: string (nullable = false)\n",
    "#  |-- nombre: string (nullable = true)\n",
    "#  |-- edad: integer (nullable = true)\n",
    "#  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Transformaciones vs Acciones (Lazy Evaluation)\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Spark usa un concepto llamado **\"lazy evaluation\"** (evaluaci√≥n perezosa):\n",
    "\n",
    "- **Transformaciones**: Definen QU√â hacer, pero NO ejecutan nada todav√≠a. Son como escribir una receta.\n",
    "  - Ejemplos: `filter()`, `select()`, `groupBy()`, `withColumn()`\n",
    "\n",
    "- **Acciones**: Ejecutan TODAS las transformaciones pendientes. Son como cocinar la receta.\n",
    "  - Ejemplos: `show()`, `count()`, `collect()`, `write()`\n",
    "\n",
    "**Analog√≠a del mundo real:** Imagina que est√°s en un restaurante. Las transformaciones son como agregar platos a tu orden (no se cocina nada a√∫n). La acci√≥n es cuando dices \"eso es todo\" y el mesero env√≠a la orden a la cocina.\n",
    "\n",
    "**¬øPor qu√© lazy?** Porque Spark puede optimizar todas las operaciones juntas antes de ejecutarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMACIONES (no ejecutan nada todavia)\n",
    "\n",
    "# Transformacion 1: Filtrar ventas de Electronica\n",
    "# filter() selecciona filas que cumplen una condicion\n",
    "df_electronica = df_ventas.filter(df_ventas[\"categoria\"] == \"Electronica\")\n",
    "\n",
    "# Transformacion 2: Agregar columna de total\n",
    "# withColumn() agrega o reemplaza una columna\n",
    "# F.col() referencia una columna existente\n",
    "df_con_total = df_electronica.withColumn(\n",
    "    \"total\",                                      # Nombre de la nueva columna\n",
    "    F.col(\"cantidad\") * F.col(\"precio_unitario\")  # Calculo: cantidad * precio\n",
    ")\n",
    "\n",
    "# Transformacion 3: Seleccionar solo algunas columnas\n",
    "# select() elige que columnas mostrar\n",
    "df_resultado = df_con_total.select(\"fecha\", \"producto\", \"total\")\n",
    "\n",
    "# NOTA: Hasta aqui, Spark NO ha procesado ningun dato\n",
    "# Solo ha guardado las instrucciones de que hacer\n",
    "\n",
    "print(\"Transformaciones definidas (aun no ejecutadas)\")\n",
    "print(\"Tipo del resultado:\", type(df_resultado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCION: Ahora si ejecutamos todo\n",
    "# show() es una ACCION que dispara la ejecucion\n",
    "\n",
    "print(\"Ejecutando transformaciones con show():\")\n",
    "df_resultado.show()\n",
    "\n",
    "# Output esperado:\n",
    "# +----------+-------+-------+\n",
    "# |     fecha|producto|  total|\n",
    "# +----------+-------+-------+\n",
    "# |2024-01-01| Laptop|30000.0|\n",
    "# |2024-01-01|  Mouse| 3500.0|\n",
    "# |2024-01-03|Monitor|24000.0|\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver el \"plan de ejecucion\" que Spark genera\n",
    "# explain() muestra como Spark planea ejecutar las transformaciones\n",
    "\n",
    "print(\"Plan de ejecucion:\")\n",
    "df_resultado.explain()\n",
    "\n",
    "# Esto muestra las operaciones que Spark realizara\n",
    "# Es util para entender y optimizar consultas complejas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Operaciones B√°sicas con DataFrames\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Las operaciones m√°s comunes en DataFrames son:\n",
    "- **select()**: Elegir columnas (como SELECT en SQL)\n",
    "- **filter()**: Filtrar filas (como WHERE en SQL)\n",
    "- **withColumn()**: Agregar o modificar columnas\n",
    "- **groupBy()**: Agrupar datos (como GROUP BY en SQL)\n",
    "- **orderBy()**: Ordenar datos (como ORDER BY en SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT: Elegir columnas especificas\n",
    "\n",
    "# Forma 1: Usando nombres de columnas como strings\n",
    "df_ventas.select(\"producto\", \"cantidad\").show(5)\n",
    "\n",
    "# Forma 2: Usando F.col() para mas flexibilidad\n",
    "df_ventas.select(\n",
    "    F.col(\"producto\"),\n",
    "    F.col(\"cantidad\"),\n",
    "    (F.col(\"cantidad\") * F.col(\"precio_unitario\")).alias(\"total\")  # Columna calculada\n",
    ").show(5)\n",
    "\n",
    "# Output esperado: tabla con producto, cantidad, y total calculado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER: Filtrar filas por condiciones\n",
    "\n",
    "# Filtro simple: cantidad mayor a 5\n",
    "print(\"Ventas con cantidad > 5:\")\n",
    "df_ventas.filter(F.col(\"cantidad\") > 5).show()\n",
    "\n",
    "# Filtros multiples: usando & (AND) y | (OR)\n",
    "# IMPORTANTE: Cada condicion debe estar entre parentesis\n",
    "print(\"Electronica con cantidad >= 5:\")\n",
    "df_ventas.filter(\n",
    "    (F.col(\"categoria\") == \"Electronica\") &  # AND\n",
    "    (F.col(\"cantidad\") >= 5)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHCOLUMN: Agregar o modificar columnas\n",
    "\n",
    "# Agregar columna de total\n",
    "df_con_total = df_ventas.withColumn(\n",
    "    \"total\",\n",
    "    F.col(\"cantidad\") * F.col(\"precio_unitario\")\n",
    ")\n",
    "\n",
    "# Agregar columna de fecha como tipo Date (conversion)\n",
    "df_con_fecha = df_con_total.withColumn(\n",
    "    \"fecha_date\",\n",
    "    F.to_date(F.col(\"fecha\"), \"yyyy-MM-dd\")  # Convertir string a fecha\n",
    ")\n",
    "\n",
    "# Agregar columna con valor constante\n",
    "df_con_region = df_con_fecha.withColumn(\n",
    "    \"region\",\n",
    "    F.lit(\"Norte\")  # F.lit() crea un valor literal/constante\n",
    ")\n",
    "\n",
    "df_con_region.show(5)\n",
    "df_con_region.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPBY + AGREGACIONES: Resumir datos\n",
    "\n",
    "# Agrupar por categoria y calcular metricas\n",
    "print(\"Resumen por categoria:\")\n",
    "df_ventas.groupBy(\"categoria\").agg(\n",
    "    F.count(\"*\").alias(\"num_ventas\"),           # Contar filas\n",
    "    F.sum(\"cantidad\").alias(\"cantidad_total\"),  # Sumar cantidades\n",
    "    F.avg(\"precio_unitario\").alias(\"precio_promedio\"),  # Promedio\n",
    "    F.max(\"precio_unitario\").alias(\"precio_max\")  # Maximo\n",
    ").show()\n",
    "\n",
    "# Agrupar por multiples columnas\n",
    "print(\"Resumen por fecha y categoria:\")\n",
    "df_ventas.groupBy(\"fecha\", \"categoria\").agg(\n",
    "    F.sum(\"cantidad\").alias(\"cantidad_total\"),\n",
    "    F.sum(F.col(\"cantidad\") * F.col(\"precio_unitario\")).alias(\"ingreso_total\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDERBY: Ordenar resultados\n",
    "\n",
    "# Ordenar por precio descendente\n",
    "print(\"Ventas ordenadas por precio (mayor a menor):\")\n",
    "df_ventas.orderBy(F.col(\"precio_unitario\").desc()).show(5)\n",
    "\n",
    "# Ordenar por multiples columnas\n",
    "print(\"Ordenado por categoria ASC, luego por cantidad DESC:\")\n",
    "df_ventas.orderBy(\n",
    "    F.col(\"categoria\").asc(),\n",
    "    F.col(\"cantidad\").desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5 ===\n",
    "## 5. Spark SQL\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Spark permite usar SQL directamente sobre DataFrames. Esto es √∫til si ya conoces SQL o prefieres esa sintaxis.\n",
    "\n",
    "**Analog√≠a del mundo real:** Es como tener dos idiomas para comunicarte: puedes usar la API de DataFrames (como Python) o SQL (como el ingl√©s del mundo de bases de datos). Ambos logran lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, registramos el DataFrame como una \"tabla temporal\"\n",
    "# Esto permite que SQL acceda a los datos\n",
    "df_ventas.createOrReplaceTempView(\"ventas\")\n",
    "\n",
    "print(\"Tabla 'ventas' registrada para SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora podemos usar SQL directamente\n",
    "# spark.sql() ejecuta consultas SQL y retorna un DataFrame\n",
    "\n",
    "# Consulta simple\n",
    "print(\"SELECT con SQL:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT producto, cantidad, precio_unitario\n",
    "    FROM ventas\n",
    "    WHERE cantidad > 5\n",
    "    ORDER BY cantidad DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta con agregaciones\n",
    "print(\"Agregacion con SQL:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        categoria,\n",
    "        COUNT(*) as num_ventas,\n",
    "        SUM(cantidad) as total_unidades,\n",
    "        ROUND(AVG(precio_unitario), 2) as precio_promedio,\n",
    "        SUM(cantidad * precio_unitario) as ingreso_total\n",
    "    FROM ventas\n",
    "    GROUP BY categoria\n",
    "    ORDER BY ingreso_total DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta con subconsulta\n",
    "print(\"Productos con precio mayor al promedio:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT producto, precio_unitario\n",
    "    FROM ventas\n",
    "    WHERE precio_unitario > (SELECT AVG(precio_unitario) FROM ventas)\n",
    "    ORDER BY precio_unitario DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 6 ===\n",
    "## 6. Lectura y Escritura de Archivos\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Spark puede leer y escribir datos en m√∫ltiples formatos:\n",
    "- **CSV**: Texto separado por comas (f√°cil de leer, pero no eficiente)\n",
    "- **JSON**: Datos estructurados (flexible pero verboso)\n",
    "- **Parquet**: Formato columnar binario (muy eficiente para Big Data)\n",
    "\n",
    "**Analog√≠a del mundo real:** Es como guardar fotos: puedes usar JPG (peque√±o pero pierde calidad), PNG (sin p√©rdida pero grande), o RAW (m√°xima calidad para profesionales). Parquet es como el RAW del mundo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las rutas de salida\n",
    "ruta_csv = \"/home/jovyan/data/sample/ventas_ejemplo.csv\"\n",
    "ruta_parquet = \"/home/jovyan/data/sample/ventas_ejemplo.parquet\"\n",
    "ruta_json = \"/home/jovyan/data/sample/ventas_ejemplo.json\"\n",
    "\n",
    "# ESCRIBIR CSV\n",
    "# mode(\"overwrite\") reemplaza si ya existe\n",
    "# header=True incluye los nombres de columnas\n",
    "df_ventas.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(ruta_csv)\n",
    "\n",
    "print(f\"CSV guardado en: {ruta_csv}\")\n",
    "\n",
    "# ESCRIBIR PARQUET (formato recomendado para Big Data)\n",
    "df_ventas.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(ruta_parquet)\n",
    "\n",
    "print(f\"Parquet guardado en: {ruta_parquet}\")\n",
    "\n",
    "# ESCRIBIR JSON\n",
    "df_ventas.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(ruta_json)\n",
    "\n",
    "print(f\"JSON guardado en: {ruta_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEER CSV\n",
    "df_desde_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(ruta_csv)\n",
    "\n",
    "print(\"Datos leidos desde CSV:\")\n",
    "df_desde_csv.show(3)\n",
    "\n",
    "# LEER PARQUET (mas rapido, preserva tipos)\n",
    "df_desde_parquet = spark.read.parquet(ruta_parquet)\n",
    "\n",
    "print(\"Datos leidos desde Parquet:\")\n",
    "df_desde_parquet.show(3)\n",
    "\n",
    "# LEER JSON\n",
    "df_desde_json = spark.read.json(ruta_json)\n",
    "\n",
    "print(\"Datos leidos desde JSON:\")\n",
    "df_desde_json.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar esquemas de cada formato\n",
    "print(\"Esquema desde CSV (inferido):\")\n",
    "df_desde_csv.printSchema()\n",
    "\n",
    "print(\"Esquema desde Parquet (preservado exacto):\")\n",
    "df_desde_parquet.printSchema()\n",
    "\n",
    "# Parquet preserva los tipos exactos, CSV necesita inferirlos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio 2.1: An√°lisis de Ventas\n",
    "\n",
    "Usando el DataFrame `df_ventas`:\n",
    "1. Calcula el ingreso total por producto (cantidad √ó precio)\n",
    "2. Encuentra el producto con mayor ingreso total\n",
    "3. Muestra solo los 3 productos principales\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `groupBy()` y `agg()` con `F.sum()`\n",
    "- Usa `orderBy()` con `.desc()` para orden descendente\n",
    "- Usa `limit(3)` para mostrar solo 3 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calcula el top 3 productos por ingreso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Top 3 productos por ingreso\n",
    "\n",
    "# Paso 1: Calcular ingreso por fila\n",
    "# withColumn agrega una columna con el calculo\n",
    "df_con_ingreso = df_ventas.withColumn(\n",
    "    \"ingreso\",\n",
    "    F.col(\"cantidad\") * F.col(\"precio_unitario\")\n",
    ")\n",
    "\n",
    "# Paso 2: Agrupar por producto y sumar ingresos\n",
    "# groupBy agrupa, agg aplica funciones de agregacion\n",
    "df_por_producto = df_con_ingreso.groupBy(\"producto\").agg(\n",
    "    F.sum(\"ingreso\").alias(\"ingreso_total\"),      # Suma de ingresos\n",
    "    F.sum(\"cantidad\").alias(\"unidades_vendidas\")  # Suma de unidades\n",
    ")\n",
    "\n",
    "# Paso 3: Ordenar descendente y limitar a 3\n",
    "top_3 = df_por_producto \\\n",
    "    .orderBy(F.col(\"ingreso_total\").desc()) \\\n",
    "    .limit(3)\n",
    "\n",
    "print(\"Top 3 productos por ingreso:\")\n",
    "top_3.show()\n",
    "\n",
    "# Output esperado:\n",
    "# +--------+-------------+-----------------+\n",
    "# |producto|ingreso_total|unidades_vendidas|\n",
    "# +--------+-------------+-----------------+\n",
    "# |  Laptop|      45000.0|                3|\n",
    "# | Monitor|      24000.0|                3|\n",
    "# |   Silla|      15000.0|                6|\n",
    "# +--------+-------------+-----------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 2.2: Filtrado Avanzado\n",
    "\n",
    "Encuentra todas las ventas que cumplan TODAS estas condiciones:\n",
    "1. Categor√≠a es \"Electronica\"\n",
    "2. Precio unitario mayor a 500\n",
    "3. Cantidad mayor o igual a 2\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `filter()` con condiciones m√∫ltiples\n",
    "- Conecta condiciones con `&` (AND)\n",
    "- Cada condici√≥n debe estar entre par√©ntesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filtra las ventas que cumplan todas las condiciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Filtrado avanzado\n",
    "\n",
    "# Definimos cada condicion\n",
    "# F.col() referencia la columna para la comparacion\n",
    "condicion_categoria = F.col(\"categoria\") == \"Electronica\"\n",
    "condicion_precio = F.col(\"precio_unitario\") > 500\n",
    "condicion_cantidad = F.col(\"cantidad\") >= 2\n",
    "\n",
    "# Aplicamos todas las condiciones con & (AND)\n",
    "df_filtrado = df_ventas.filter(\n",
    "    condicion_categoria & \n",
    "    condicion_precio & \n",
    "    condicion_cantidad\n",
    ")\n",
    "\n",
    "print(\"Ventas filtradas (Electronica, precio>500, cantidad>=2):\")\n",
    "df_filtrado.show()\n",
    "\n",
    "# Forma alternativa en una sola expresion:\n",
    "df_ventas.filter(\n",
    "    (F.col(\"categoria\") == \"Electronica\") &\n",
    "    (F.col(\"precio_unitario\") > 500) &\n",
    "    (F.col(\"cantidad\") >= 2)\n",
    ").show()\n",
    "\n",
    "# Output esperado:\n",
    "# +----------+-------+-----------+--------+---------------+\n",
    "# |     fecha|producto|  categoria|cantidad|precio_unitario|\n",
    "# +----------+-------+-----------+--------+---------------+\n",
    "# |2024-01-01| Laptop|Electronica|       2|        15000.0|\n",
    "# |2024-01-03|Monitor|Electronica|       3|         8000.0|\n",
    "# |2024-01-03|Teclado|Electronica|       8|          800.0|\n",
    "# +----------+-------+-----------+--------+---------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 2.3: Spark SQL\n",
    "\n",
    "Usando Spark SQL (no la API de DataFrame), escribe una consulta que:\n",
    "1. Calcule el ingreso total por fecha\n",
    "2. Ordene por fecha\n",
    "3. Muestre fecha e ingreso total\n",
    "\n",
    "**Pistas:**\n",
    "- La tabla ya est√° registrada como \"ventas\"\n",
    "- Usa `SUM(cantidad * precio_unitario)`\n",
    "- Usa `GROUP BY fecha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escribe la consulta SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Ingreso por fecha con SQL\n",
    "\n",
    "resultado_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        fecha,\n",
    "        SUM(cantidad * precio_unitario) as ingreso_total\n",
    "    FROM ventas\n",
    "    GROUP BY fecha\n",
    "    ORDER BY fecha\n",
    "\"\"\")\n",
    "\n",
    "print(\"Ingreso total por fecha:\")\n",
    "resultado_sql.show()\n",
    "\n",
    "# Output esperado:\n",
    "# +----------+-------------+\n",
    "# |     fecha|ingreso_total|\n",
    "# +----------+-------------+\n",
    "# |2024-01-01|      33500.0|\n",
    "# |2024-01-02|      15000.0|\n",
    "# |2024-01-03|      30400.0|\n",
    "# |2024-01-04|      18000.0|\n",
    "# |2024-01-05|      10250.0|\n",
    "# +----------+-------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **Lazy Evaluation**: Spark no ejecuta nada hasta que hay una acci√≥n. Las transformaciones son \"recetas\", las acciones son \"cocinar\".\n",
    "- **Transformaciones**: `select()`, `filter()`, `withColumn()`, `groupBy()`, `orderBy()` - definen qu√© hacer\n",
    "- **Acciones**: `show()`, `count()`, `collect()`, `write()` - ejecutan el plan\n",
    "- **Spark SQL**: Permite usar SQL est√°ndar sobre DataFrames\n",
    "- **Formatos de archivo**: CSV (legible), Parquet (eficiente), JSON (flexible)\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **Amazon EMR**: Ejecuta estas mismas operaciones en clusters grandes\n",
    "- **AWS Glue**: Servicio ETL que usa PySpark internamente\n",
    "- **Amazon Athena**: Usa SQL similar a Spark SQL sobre datos en S3\n",
    "- **Amazon S3**: Almacena archivos Parquet/CSV que Spark puede leer\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `labs/01_data_fundamentals/` para aprender sobre las 5 Vs del Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza final (opcional)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Eliminar archivos de ejemplo creados\n",
    "for ruta in [ruta_csv, ruta_parquet, ruta_json]:\n",
    "    if os.path.exists(ruta):\n",
    "        shutil.rmtree(ruta)\n",
    "        print(f\"Eliminado: {ruta}\")\n",
    "\n",
    "print(\"\\nLimpieza completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
