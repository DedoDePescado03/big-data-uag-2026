# üìä Lab 01: Fundamentos de Big Data

## üéØ Descripci√≥n

Este laboratorio introduce los conceptos fundamentales de Big Data utilizando el dataset de viajes en taxi de Nueva York. Aprender√°s las **5 Vs del Big Data**, tipos de datos, y dar√°s tus primeros pasos con PySpark.

## üìö M√≥dulo AWS Academy Relacionado

**M√≥dulo 3: Data Characteristics** - Caracter√≠sticas de los datos, las 5 Vs, tipos de datos estructurados y no estructurados.

## ‚è±Ô∏è Tiempo Estimado

**90-120 minutos** (incluyendo ejercicios pr√°cticos)

## üìã Prerequisitos

- ‚úÖ Haber completado el Lab 00 (Setup del entorno)
- ‚úÖ Docker Desktop instalado y funcionando
- ‚úÖ Conocimientos b√°sicos de Python

---

## üöÄ Instrucciones de Inicio

### Paso 1: Navegar al Directorio del Proyecto

```bash
# Abrir terminal y navegar al proyecto
cd ~/Documents/big-data-uag-2026
```

### Paso 2: Levantar el Cluster de Spark

```bash
# Construir y levantar los contenedores (primera vez tomar√° unos minutos)
docker compose -f infrastructure/docker-compose.spark.yml up -d --build
```

**üí° Tip:** La primera vez que ejecutes este comando, Docker descargar√° las im√°genes base (~2-3 GB). Esto solo ocurre una vez.

### Paso 3: Verificar que los Servicios Est√°n Corriendo

```bash
# Ver el estado de los contenedores
docker compose -f infrastructure/docker-compose.spark.yml ps
```

Deber√≠as ver algo como:

```
NAME            STATUS      PORTS
spark-master    running     0.0.0.0:7077->7077/tcp, 0.0.0.0:8080->8080/tcp
spark-worker    running     0.0.0.0:8081->8081/tcp
jupyter-spark   running     0.0.0.0:4040->4040/tcp, 0.0.0.0:8888->8888/tcp
```

### Paso 4: Acceder a Jupyter Lab

1. Abre tu navegador web
2. Ve a: **http://localhost:8888**
3. No se requiere contrase√±a (el token est√° deshabilitado para desarrollo local)

### Paso 5: Abrir el Notebook del Laboratorio

1. En Jupyter Lab, navega a: `labs/01_data_fundamentals/`
2. Abre el archivo: `01_introduccion_big_data.ipynb`

---

## üåê URLs Importantes

| Servicio | URL | Descripci√≥n |
|----------|-----|-------------|
| **Jupyter Lab** | http://localhost:8888 | Entorno de notebooks |
| **Spark Master UI** | http://localhost:8080 | Dashboard del cluster |
| **Spark Worker UI** | http://localhost:8081 | Estado del worker |
| **Spark App UI** | http://localhost:4040 | Jobs en ejecuci√≥n (solo cuando hay una app corriendo) |

---

## üìÅ Estructura del Laboratorio

```
01_data_fundamentals/
‚îú‚îÄ‚îÄ README.md                          # Este archivo (instrucciones)
‚îú‚îÄ‚îÄ 01_introduccion_big_data.ipynb     # Notebook principal
‚îî‚îÄ‚îÄ data/                              # Datos de muestra
    ‚îî‚îÄ‚îÄ sample_taxi_trips.csv
```

---

## üìä Dataset

### Opci√≥n A: Datos de Muestra (Recomendado para empezar)

El notebook incluye c√≥digo para generar datos de muestra que simulan viajes en taxi. Esto permite practicar sin necesidad de descargar el dataset completo.

### Opci√≥n B: Dataset Completo de Kaggle

Para trabajar con el dataset real de NYC Taxi (1.5M+ registros):

```bash
# Instalar Kaggle CLI (si no lo tienes)
pip install kaggle

# Configurar credenciales de Kaggle
# 1. Ve a kaggle.com/account
# 2. Click en "Create New API Token"
# 3. Mueve el archivo descargado:
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Descargar el dataset del Lab 01
python infrastructure/scripts/download-datasets.py --lab 01
```

---

## üõë Detener el Cluster

Cuando termines de trabajar:

```bash
# Detener los contenedores (preserva los datos)
docker compose -f infrastructure/docker-compose.spark.yml stop

# O para detener y eliminar los contenedores
docker compose -f infrastructure/docker-compose.spark.yml down
```

**‚ö†Ô∏è Advertencia:** Si usas `down -v`, tambi√©n se eliminar√°n los vol√∫menes con tus datos.

---

## üîß Soluci√≥n de Problemas

### El puerto 8888 ya est√° en uso

```bash
# Verificar qu√© proceso usa el puerto
lsof -i :8888

# Detener el proceso o cambiar el puerto en docker-compose.spark.yml
```

### Los contenedores no inician

```bash
# Ver los logs para identificar el problema
docker compose -f infrastructure/docker-compose.spark.yml logs

# Reiniciar Docker Desktop y volver a intentar
```

### Jupyter no conecta con Spark

```bash
# Verificar que spark-master est√° corriendo
docker compose -f infrastructure/docker-compose.spark.yml logs spark-master

# Reiniciar el cluster
docker compose -f infrastructure/docker-compose.spark.yml restart
```

### Error de memoria en Spark

El worker est√° configurado con 2GB de RAM. Si procesas datasets muy grandes:

1. Aumenta la memoria en `docker-compose.spark.yml`:
   ```yaml
   SPARK_WORKER_MEMORY=4g
   ```
2. Reinicia el cluster

---

## üìù Contenido del Laboratorio

### Secci√≥n 1: Introducci√≥n a Big Data
- ¬øQu√© es Big Data?
- Las 5 Vs explicadas con ejemplos del mundo real

### Secci√≥n 2: Tipos de Datos
- Estructurados vs Semi-estructurados vs No estructurados
- Ejemplos pr√°cticos

### Secci√≥n 3: Primeros Pasos con PySpark
- Crear una SparkSession
- Cargar datos en DataFrames
- Operaciones b√°sicas

### Secci√≥n 4: Exploraci√≥n del Dataset de Taxis
- Estad√≠sticas descriptivas
- Detecci√≥n de valores nulos
- Visualizaciones b√°sicas

### Secci√≥n 5: Ejercicios Pr√°cticos
- 3 ejercicios con soluciones completas
- Conexi√≥n con conceptos de AWS

---

## ‚úÖ Checklist de Finalizaci√≥n

- [ ] Puedo explicar las 5 Vs del Big Data con ejemplos
- [ ] Entiendo la diferencia entre datos estructurados y no estructurados
- [ ] Puedo crear una SparkSession y cargar datos
- [ ] Puedo realizar operaciones b√°sicas con DataFrames
- [ ] Complet√© los 3 ejercicios pr√°cticos

---

## ‚û°Ô∏è Siguiente Paso

Una vez completado este laboratorio, contin√∫a con:
**Lab 02: ETL Pipeline** (`labs/02_etl_pipeline/`)
