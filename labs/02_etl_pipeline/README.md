# MÃ³dulo 02: Pipeline ETL

## DescripciÃ³n

Este mÃ³dulo enseÃ±a cÃ³mo construir pipelines ETL (Extract, Transform, Load) usando PySpark. AprenderÃ¡s a extraer datos de mÃºltiples fuentes, transformarlos y cargarlos en diferentes destinos.

## Objetivos de Aprendizaje

Al completar este mÃ³dulo podrÃ¡s:
- Entender las diferencias entre ETL y ELT
- Extraer datos de CSV, JSON, Parquet y bases de datos
- Aplicar transformaciones de limpieza y enriquecimiento
- Cargar datos en diferentes formatos y destinos
- Manejar errores y datos problemÃ¡ticos

## MÃ³dulo AWS Academy Relacionado

ðŸ“š **MÃ³dulo 6: Data Processing and Analysis**
- ETL/ELT concepts
- AWS Glue para ETL serverless
- Data wrangling

## Dataset

**Brazilian E-Commerce (Olist)**
- Fuente: Kaggle `olistbr/brazilian-ecommerce`
- Datos reales de e-commerce de Brasil
- MÃºltiples tablas relacionadas (pedidos, productos, clientes, pagos)

## Contenido

| Notebook | DescripciÃ³n | Tiempo |
|----------|-------------|--------|
| `01_etl_concepts.ipynb` | Conceptos ETL vs ELT | 30 min |
| `02_data_extraction.ipynb` | ExtracciÃ³n de mÃºltiples fuentes | 45 min |
| `03_data_transformation.ipynb` | Transformaciones comunes | 60 min |
| `04_data_loading.ipynb` | Carga a diferentes destinos | 45 min |
| `05_pipeline_completo.ipynb` | Pipeline ETL end-to-end | 60 min |

## Conceptos Clave

### ETL vs ELT

```
ETL (Extract-Transform-Load):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract â”‚ -> â”‚  Transform  â”‚ -> â”‚  Load   â”‚
â”‚ (Source)â”‚    â”‚ (Spark/ETL) â”‚    â”‚ (Target)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
- Transformaciones ANTES de cargar
- Usado cuando el destino tiene capacidad limitada

ELT (Extract-Load-Transform):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract â”‚ -> â”‚  Load   â”‚ -> â”‚  Transform  â”‚
â”‚ (Source)â”‚    â”‚ (Lake)  â”‚    â”‚ (Lake/DW)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
- Carga datos crudos primero
- Transformaciones en el data lake/warehouse
- Aprovecha poder de cÃ³mputo del destino
```

### Fases del Pipeline

1. **Extract (ExtracciÃ³n)**
   - Conectar a fuentes de datos
   - Validar conexiones
   - Manejar errores de lectura

2. **Transform (TransformaciÃ³n)**
   - Limpieza (nulos, duplicados)
   - NormalizaciÃ³n (formatos, tipos)
   - Enriquecimiento (joins, cÃ¡lculos)
   - AgregaciÃ³n (resÃºmenes)

3. **Load (Carga)**
   - Elegir formato destino
   - Particionar datos
   - Validar carga exitosa

## Comandos Ãštiles

```bash
# Descargar dataset de Kaggle
python infrastructure/scripts/download-datasets.py --lab 02

# Verificar archivos descargados
ls -la data/raw/02_etl_pipeline/

# Iniciar cluster
./infrastructure/scripts/start-cluster.sh spark
```

## Estructura de Datos Olist

```
olist_customers_dataset.csv       - Clientes
olist_orders_dataset.csv          - Pedidos
olist_order_items_dataset.csv     - Items de pedidos
olist_order_payments_dataset.csv  - Pagos
olist_products_dataset.csv        - Productos
olist_sellers_dataset.csv         - Vendedores
product_category_name_translation.csv - Traducciones
```

## ConexiÃ³n con AWS

| Concepto Local | Servicio AWS | Uso |
|----------------|--------------|-----|
| PySpark ETL | AWS Glue | ETL serverless |
| Script Python | Glue Job | CÃ³digo ETL |
| Schema inference | Glue Crawler | Descubrir esquemas |
| CSV/Parquet | S3 | Almacenamiento |
| Transformaciones | Glue Studio | ETL visual |

## Siguiente MÃ³dulo

DespuÃ©s de completar ETL, continÃºa con:
- `03_batch_processing/` - Procesamiento batch
