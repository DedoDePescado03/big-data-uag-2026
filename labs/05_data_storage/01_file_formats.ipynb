{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatos de Archivo para Big Data\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Conocer los principales formatos de archivo\n",
    "- Comparar rendimiento entre formatos\n",
    "- Elegir el formato adecuado para cada caso\n",
    "- Configurar compresi√≥n y opciones de escritura\n",
    "\n",
    "## Prerequisitos\n",
    "- `00_setup/02_spark_basics.ipynb`\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 45 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 8: Data Storage - Formatos optimizados para S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FileFormats\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark listo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos de prueba\n",
    "# 100,000 registros para comparar formatos\n",
    "\n",
    "datos = [(i, \n",
    "          f\"producto_{i % 1000}\", \n",
    "          f\"categoria_{i % 50}\",\n",
    "          float(i * 1.5),\n",
    "          i % 100,\n",
    "          f\"descripcion larga del producto numero {i} con texto adicional\"\n",
    "         ) for i in range(100000)]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    datos,\n",
    "    [\"id\", \"producto\", \"categoria\", \"precio\", \"stock\", \"descripcion\"]\n",
    ")\n",
    "\n",
    "print(f\"Registros: {df.count():,}\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. Formatos de Texto (CSV, JSON)\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "**CSV (Comma-Separated Values)**\n",
    "- ‚úÖ Simple, legible, universal\n",
    "- ‚ùå Sin esquema, sin compresi√≥n nativa, lento\n",
    "\n",
    "**JSON (JavaScript Object Notation)**\n",
    "- ‚úÖ Esquema flexible, anidado, legible\n",
    "- ‚ùå Verboso, grande, parsing lento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de salida\n",
    "base_path = \"/home/jovyan/data/sample/formatos\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# Escribir CSV\n",
    "start = time.time()\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{base_path}/csv\")\n",
    "tiempo_csv_write = time.time() - start\n",
    "\n",
    "# Escribir JSON\n",
    "start = time.time()\n",
    "df.write.mode(\"overwrite\").json(f\"{base_path}/json\")\n",
    "tiempo_json_write = time.time() - start\n",
    "\n",
    "print(f\"CSV escritura: {tiempo_csv_write:.2f}s\")\n",
    "print(f\"JSON escritura: {tiempo_json_write:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer CSV\n",
    "start = time.time()\n",
    "df_csv = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{base_path}/csv\")\n",
    "_ = df_csv.count()  # Forzar lectura\n",
    "tiempo_csv_read = time.time() - start\n",
    "\n",
    "# Leer JSON\n",
    "start = time.time()\n",
    "df_json = spark.read.json(f\"{base_path}/json\")\n",
    "_ = df_json.count()\n",
    "tiempo_json_read = time.time() - start\n",
    "\n",
    "print(f\"CSV lectura: {tiempo_csv_read:.2f}s\")\n",
    "print(f\"JSON lectura: {tiempo_json_read:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Formatos Columnares (Parquet, ORC)\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "**Parquet**\n",
    "- ‚úÖ Columnar, comprimido, r√°pido para analytics\n",
    "- ‚úÖ Esquema embebido, predicate pushdown\n",
    "- El est√°ndar para data lakes\n",
    "\n",
    "**ORC (Optimized Row Columnar)**\n",
    "- ‚úÖ Similar a Parquet, optimizado para Hive\n",
    "- ‚úÖ Mejor compresi√≥n en algunos casos\n",
    "\n",
    "**¬øPor qu√© columnar es mejor para analytics?**\n",
    "- Solo lee columnas necesarias\n",
    "- Mejor compresi√≥n (datos similares juntos)\n",
    "- Estad√≠sticas por columna para filtrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir Parquet\n",
    "start = time.time()\n",
    "df.write.mode(\"overwrite\").parquet(f\"{base_path}/parquet\")\n",
    "tiempo_parquet_write = time.time() - start\n",
    "\n",
    "# Escribir ORC\n",
    "start = time.time()\n",
    "df.write.mode(\"overwrite\").orc(f\"{base_path}/orc\")\n",
    "tiempo_orc_write = time.time() - start\n",
    "\n",
    "print(f\"Parquet escritura: {tiempo_parquet_write:.2f}s\")\n",
    "print(f\"ORC escritura: {tiempo_orc_write:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer Parquet\n",
    "start = time.time()\n",
    "df_parquet = spark.read.parquet(f\"{base_path}/parquet\")\n",
    "_ = df_parquet.count()\n",
    "tiempo_parquet_read = time.time() - start\n",
    "\n",
    "# Leer ORC\n",
    "start = time.time()\n",
    "df_orc = spark.read.orc(f\"{base_path}/orc\")\n",
    "_ = df_orc.count()\n",
    "tiempo_orc_read = time.time() - start\n",
    "\n",
    "print(f\"Parquet lectura: {tiempo_parquet_read:.2f}s\")\n",
    "print(f\"ORC lectura: {tiempo_orc_read:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventaja de columnar: leer solo columnas necesarias\n",
    "print(\"Lectura selectiva de columnas:\")\n",
    "\n",
    "# CSV tiene que leer todo el archivo\n",
    "start = time.time()\n",
    "df_csv.select(\"id\", \"precio\").filter(F.col(\"precio\") > 1000).count()\n",
    "tiempo_csv_select = time.time() - start\n",
    "\n",
    "# Parquet solo lee columnas id y precio\n",
    "start = time.time()\n",
    "df_parquet.select(\"id\", \"precio\").filter(F.col(\"precio\") > 1000).count()\n",
    "tiempo_parquet_select = time.time() - start\n",
    "\n",
    "print(f\"CSV (lee todo): {tiempo_csv_select:.3f}s\")\n",
    "print(f\"Parquet (lee 2 columnas): {tiempo_parquet_select:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Comparaci√≥n de Tama√±os\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "El tama√±o del archivo impacta directamente en:\n",
    "- Costos de almacenamiento (S3)\n",
    "- Tiempo de transferencia\n",
    "- Velocidad de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Obtener tama√±o de directorio en bytes\"\"\"\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "# Calcular tama√±os\n",
    "tamanos = {\n",
    "    \"CSV\": get_dir_size(f\"{base_path}/csv\"),\n",
    "    \"JSON\": get_dir_size(f\"{base_path}/json\"),\n",
    "    \"Parquet\": get_dir_size(f\"{base_path}/parquet\"),\n",
    "    \"ORC\": get_dir_size(f\"{base_path}/orc\")\n",
    "}\n",
    "\n",
    "print(\"Comparaci√≥n de tama√±os:\")\n",
    "print(\"-\" * 40)\n",
    "for formato, tamano in sorted(tamanos.items(), key=lambda x: x[1]):\n",
    "    mb = tamano / (1024 * 1024)\n",
    "    ratio = tamano / tamanos[\"CSV\"]\n",
    "    print(f\"{formato:10} {mb:8.2f} MB  ({ratio:.1%} vs CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Resumen Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla resumen\n",
    "resumen = spark.createDataFrame([\n",
    "    (\"CSV\", tiempo_csv_write, tiempo_csv_read, tamanos[\"CSV\"]/(1024*1024)),\n",
    "    (\"JSON\", tiempo_json_write, tiempo_json_read, tamanos[\"JSON\"]/(1024*1024)),\n",
    "    (\"Parquet\", tiempo_parquet_write, tiempo_parquet_read, tamanos[\"Parquet\"]/(1024*1024)),\n",
    "    (\"ORC\", tiempo_orc_write, tiempo_orc_read, tamanos[\"ORC\"]/(1024*1024))\n",
    "], [\"formato\", \"escritura_seg\", \"lectura_seg\", \"tamano_mb\"])\n",
    "\n",
    "print(\"RESUMEN COMPARATIVO:\")\n",
    "resumen.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio F.1: Elegir Formato\n",
    "\n",
    "¬øQu√© formato elegir√≠as para cada caso?\n",
    "1. Exportar datos para Excel\n",
    "2. Data lake para analytics con Athena\n",
    "3. Intercambio con una API REST\n",
    "4. Streaming de eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respuestas\n",
    "print(\"1. Excel: CSV - Universal, f√°cil de abrir\")\n",
    "print(\"2. Athena: Parquet - Columnar, optimizado para queries\")\n",
    "print(\"3. API REST: JSON - Est√°ndar para APIs\")\n",
    "print(\"4. Streaming: Avro - Esquema evoluci√≥n, compacto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **CSV/JSON**: Texto, legible, lento, grande\n",
    "- **Parquet/ORC**: Columnar, comprimido, r√°pido para analytics\n",
    "- **Parquet** es el est√°ndar para data lakes\n",
    "- Formato columnar = solo lee columnas necesarias\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **S3**: Almacena todos los formatos\n",
    "- **Athena**: Optimizado para Parquet/ORC en S3\n",
    "- **Glue**: Puede convertir entre formatos\n",
    "- **Redshift Spectrum**: Lee Parquet directamente\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `02_partitioning.ipynb` para estrategias de particionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza\n",
    "import shutil\n",
    "if os.path.exists(base_path):\n",
    "    shutil.rmtree(base_path)\n",
    "print(\"Archivos de prueba eliminados\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
